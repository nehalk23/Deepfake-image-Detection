# -*- coding: utf-8 -*-
"""mini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp8B6kgYtW7zEMM-GweeDbmCq0Z9g_68
"""

!pip install streamlit

!pip install streamlit transformers pillow torch

pip install tensorflow opencv-python-headless joblib streamlit

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import AutoImageProcessor, AutoModelForImageClassification
# from PIL import Image
# import torch
# 
# # Load the model and processor
# @st.cache_resource
# def load_model_and_processor():
#     processor = AutoImageProcessor.from_pretrained("prithivMLmods/Deep-Fake-Detector-Model")
#     model = AutoModelForImageClassification.from_pretrained("prithivMLmods/Deep-Fake-Detector-Model")
#     return processor, model
# 
# processor, model = load_model_and_processor()
# 
# # Function to process the image and predict
# def detect_deepfake(image):
#     try:
#         # Preprocess the image
#         inputs = processor(images=image, return_tensors="pt")
# 
#         # Perform inference
#         outputs = model(**inputs)
# 
#         # Get predicted label
#         logits = outputs.logits
#         predicted_class = torch.argmax(logits, dim=-1).item()
# 
#         # Load the labels
#         labels = model.config.id2label
#         label = labels.get(predicted_class, "Unknown")
# 
#         return label
#     except Exception as e:
#         return f"Error: {str(e)}"
# 
# # Streamlit app layout
# st.title("Deepfake Image Detector")
# st.write("Upload an image to determine if it is a deepfake or not.")
# 
# # Image upload widget
# uploaded_file = st.file_uploader("Choose an image file", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     try:
#         # Load and display the image
#         image = Image.open(uploaded_file).convert("RGB")
#         st.image(image, caption="Uploaded Image", use_column_width=True)
# 
#         # Predict and display the result
#         with st.spinner("Analyzing the image..."):
#             result = detect_deepfake(image)
# 
#         st.success(f"Prediction: {result}")
#     except Exception as e:
#         st.error(f"Error processing the image: {str(e)}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import numpy as np
# import cv2
# import streamlit as st
# import tensorflow as tf
# from tensorflow.keras.preprocessing.image import img_to_array
# from tensorflow.keras.models import load_model
# 
# # Disable GPU (if not needed)
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
# 
# # Load the trained TensorFlow model
# model_path = '/content/svm_classifier.pkl'  # Path to your .h5 file
# model = load_model(model_path)
# 
# # Ensure the model is compiled with necessary configurations
# model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
# 
# # Function to preprocess the input image
# def preprocess_image(image):
#     """Resize and preprocess the input image for the model."""
#     img = cv2.resize(image, (224, 224))  # Resize image to match model input
#     img = img_to_array(img)  # Convert to array
#     img = np.expand_dims(img, axis=0)  # Add batch dimension
#     img = img / 255.0  # Normalize pixel values to [0, 1]
#     return img
# 
# # Streamlit UI
# st.title("Real or Fake Image Detection")
# st.write("Upload an image to classify it as either 'Real' or 'Fake'.")
# 
# # Image upload
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "png", "jpeg"])
# 
# if uploaded_file is not None:
#     # Read the uploaded image as an OpenCV image
#     file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
#     img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
# 
#     # Display the uploaded image
#     st.image(img, channels="BGR", use_container_width=True)
# 
#     # Preprocess the image
#     processed_img = preprocess_image(img)
# 
#     # Predict using the trained model
#     prediction = model.predict(processed_img)
# 
#     # Debugging: check the shape of prediction
#     st.write("Prediction Shape:", prediction.shape)  # Display the shape of the prediction
# 
#     # Flatten the prediction output if necessary (e.g., model output is not scalar)
#     if len(prediction.shape) > 2:  # In case of multi-dimensional output
#         prediction = prediction.flatten()
# 
#     # Assuming output > 0.5 indicates FAKE (you might need to check if your model uses 1 class or two)
#     if prediction[0] > 0.5:  # Check the first element of the flattened prediction array
#         st.error("The image is FAKE.")
#     else:
#         st.success("The image is REAL.")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import numpy as np
# import cv2
# import streamlit as st
# import tensorflow as tf
# from tensorflow.keras.preprocessing.image import img_to_array
# from tensorflow.keras.models import load_model
# 
# # Load the trained TensorFlow model
# model_path = '/content/model.pth'  # Path to your .h5 file
# model = load_model(model_path)
# 
# # Function to preprocess the input image
# def preprocess_image(image):
#     """Resize and preprocess the input image for the model."""
#     img = cv2.resize(image, (224, 224))  # Resize image to match model input
#     img = img_to_array(img)  # Convert to array
#     img = np.expand_dims(img, axis=0)  # Add batch dimension
#     img = img / 255.0  # Normalize pixel values to [0, 1]
#     return img
# 
# # Streamlit UI
# st.title("Real or Fake Image Detection")
# st.write("Upload an image to classify it as either 'Real' or 'Fake'.")
# 
# # Image upload
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "png", "jpeg"])
# 
# if uploaded_file is not None:
#     # Read the uploaded image as an OpenCV image
#     file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
#     img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
# 
#     # Display the uploaded image
#     st.image(img, channels="BGR", use_container_width=True)  # Use use_container_width instead of use_column_width
# 
#     # Preprocess the image
#     processed_img = preprocess_image(img)
# 
#     # Predict using the trained model
#     prediction = model.predict(processed_img)
#     st.write("Raw Prediction Output:", prediction)
# 
#     # Ensure prediction is interpreted correctly
#     if isinstance(prediction, (np.ndarray, list)):  # Handle array or list output
#         predicted_value = prediction[0][0]  # Access the scalar value assuming shape (1, 1)
#     else:
#         predicted_value = prediction  # If already a scalar
# 
#     # Interpret and display the result
#     if predicted_value > 0.5:  # Assuming output > 0.5 indicates FAKE
#         st.error("The image is FAKE.")
#     else:
#         st.success("The image is REAL.")
#

import tensorflow as tf
import numpy as np
import os
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Set paths to your dataset
dataset_path = "/content/drive/MyDrive/Dataset"  # Replace with the path to your dataset
train_dir = os.path.join(dataset_path, "/content/drive/MyDrive/Dataset/Train")
val_dir = os.path.join(dataset_path, "/content/drive/MyDrive/Dataset/Test")


# Step 1: Load and Preprocess the Dataset
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,  # Normalize images to the range [0, 1]
    rotation_range=20,
    zoom_range=0.15,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    horizontal_flip=True,
    fill_mode="nearest"
)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Load images in batches
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),  # Resize images to 224x224 for the CNN
    batch_size=32,
    class_mode="binary"  # Binary classification: real vs fake
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary"
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Input

# Initialize the model
model = Sequential()

# Define the input layer explicitly using Input()
model.add(Input(shape=(224, 224, 3)))  # Instead of passing input_shape to Conv2D

# Add convolutional layers with Batch Normalization
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Batch Normalization
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Batch Normalization
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Batch Normalization
model.add(MaxPooling2D(pool_size=(2, 2)))

# Additional convolutional layers (hidden layers) with Batch Normalization
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Batch Normalization
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(BatchNormalization())  # Batch Normalization
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output from convolutional layers
model.add(Flatten())

# Add fully connected (dense) layers with Batch Normalization
model.add(Dense(1024, activation='relu'))  # Increased number of neurons
model.add(BatchNormalization())  # Batch Normalization
model.add(Dropout(0.5))  # Dropout layer to prevent overfitting

model.add(Dense(512, activation='relu'))  # Another dense layer for better learning
model.add(BatchNormalization())  # Batch Normalization
model.add(Dropout(0.5))  # Dropout to prevent overfitting

model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification (real/fake)

# Step 3: Compile the Model
model.compile(optimizer=Adam(learning_rate=0.0001), loss="binary_crossentropy", metrics=["accuracy"])

# Step 4: Train the Model
# Add EarlyStopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    epochs=10,
    callbacks=[early_stop]
)

# Step 5: Save the Trained Model
model.save("deepfake_detection_cnn.h5")

# Step 6: Evaluate the Model
loss, accuracy = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)
print(f"Validation Loss: {loss:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")